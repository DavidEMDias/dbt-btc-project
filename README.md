## üß† Summary

This project consists of a data pipeline for ingesting, transforming, and delivering Bitcoin transaction data, with a focus on detecting ‚Äúwhales‚Äù ‚Äî that is, movements of large amounts.  
The data is loaded into **Snowflake**, transformed using **dbt (core)** following **CI/CD practices (via GitHub Actions)**, and exposed for visual consumption through a dashboard (e.g., **Looker Studio**).

---

## üéØ Main Goals

1. Extract public Bitcoin transaction data hosted in a public **AWS S3** bucket.  
2. Load this data into **Snowflake** as a raw source.  
3. Identify transactions with large volumes (**whales**).  
4. Apply transformations and modeling using **dbt core** to generate clean, analysis-ready tables.  
5. Automate the entire workflow with **GitHub Actions** to ensure CI/CD (testing, builds, deployment).  
6. Make the transformed data available in **Snowflake** for use in dashboards built with **Looker Studio** (or another BI tool).  

The result is an **end-to-end data infrastructure** for analyzing large Bitcoin transactions.

## üèóÔ∏è Architecture

<img width="800" height="593" alt="image" src="https://github.com/user-attachments/assets/6b17d0cd-81a9-42e6-a6a2-ee5cd5e1fe01" />


## üõ† Technologies / Dependencies

| Technology / Tool | Purpose |
|--------------------|----------|
| **dbt (core)** | Data modeling, transformations, and orchestration |
| **Snowflake** | Data warehouse / final destination for transformed data |
| **GitHub** | Version control, collaboration, and repository management |
| **GitHub Actions** | Automation, CI/CD workflows |
| **AWS S3** | Public repository containing Bitcoin transaction data (data source) |
| **Python libraries** (`pandas`, `simplejson`) / helper scripts | Ingestion pipeline support |

## üß© Techniques & Best Practices used

| Technique | Description |
|------------|--------------|
| **Data Quality Tests (singular & generic)** | Ensures data reliability by validating integrity, uniqueness, and referential consistency through both custom (singular) and reusable (generic) dbt tests. |
| **Modularization** | Structured models in layered architecture ‚Äî **staging ‚Üí marts** ‚Äî improving readability, maintainability, and performance. |
| **Documentation in dbt** | Auto-generate browsable documentation via `dbt docs generate`. |
| **Incremental Models** | Implemented incremental logic in dbt models to process only new or updated records, improving performance and efficiency. |
| **Data Enrichment via dbt Seeds** | Enriched transactional data using static seed files for reference mappings and additional metadata. |
| **Incremental Strategies (Merge & Append)** | Applied `merge` for upserts and `append` for log-based data, depending on data characteristics and freshness needs. |
| **Python Models** | Leveraged Python-based dbt models for complex data transformations or non-SQL operations within the same dbt workflow. |
| **dbt Exposure** | Created **exposures** to explicitly link dbt models to the final **dashboard**, ensuring traceability and visibility of data lineage. |
| **Model Contracts** | Defined **model contracts** to guarantee schema stability; versioning via **dbt versions** was used to manage controlled model evolution. |
| **Continuous Integration (GitHub Actions)** | Implemented CI/CD pipelines with automated testing, linting, and deployment using GitHub Actions to ensure stable, production-ready releases. |
