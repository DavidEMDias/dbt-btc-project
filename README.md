## ğŸ§  Summary

This project consists of a data pipeline for ingesting, transforming, and delivering Bitcoin transaction data, with a focus on detecting â€œwhalesâ€ â€” that is, movements of large amounts.  
The data is loaded into **Snowflake**, transformed using **dbt (core)** following **CI/CD practices (via GitHub Actions)**, and exposed for visual consumption through a dashboard (e.g., **Looker Studio**).

---

## ğŸ¯ Main Goals

1. Extract public Bitcoin transaction data hosted in a public **AWS S3** bucket.  
2. Load this data into **Snowflake** as a raw source.  
3. Identify transactions with large volumes (**whales**).  
4. Apply transformations and modeling using **dbt core** to generate clean, analysis-ready tables.  
5. Automate the entire workflow with **GitHub Actions** to ensure CI/CD (testing, builds, deployment).  
6. Make the transformed data available in **Snowflake** for use in dashboards built with **Looker Studio** (or another BI tool).  

The result is an **end-to-end data infrastructure** for analyzing large Bitcoin transactions.

## ğŸ—ï¸ Architecture

<img width="800" height="576" alt="image" src="https://github.com/user-attachments/assets/39016e64-99e1-4bf8-85df-6dd82f2fb47f" />


### ğŸ’¡ **Architecture Explanation**

- **AWS S3** â†’ Public Bitcoin transaction data source (Parquet). 
- **Snowflake (Raw)** â†’ Stores raw transaction data exactly as ingested.  
- **dbt (Core)** â†’ Applies transformations, modular modeling, and data quality tests.  
- **Snowflake (Marts)** â†’ Final clean, analysis-ready tables.  
- **GitHub Actions** â†’ Runs CI pipeline: tests, builds, and deployments.  
- **Looker Studio** â†’ Visualization and analysis layer consuming Snowflake data.

**Source/Ingestion:** Consuming data from an S3 Bucket (the origin of the raw data).
**Load/Move:** Loading the data into Snowflake (the destination for raw storage and initial processing).
**Transform/Process:** Transforming the data with dbt (Data Build Tool) within Snowflake (the core processing logic). 
**Destination/Storage:** Materializing the final tables in Snowflake (the destination where clean, ready-to-use data resides).

ELT (Extract, Load, Transform) paradigm:
**Extract (E):** Data is read from the S3 Bucket.
**Load (L):** Data is loaded into the Snowflake data warehouse.
**Transform (T):** Data is transformed inside the data warehouse using dbt, leveraging Snowflake's compute power.  
  
## ğŸ›  Technologies / Dependencies

| Technology / Tool | Purpose |
|--------------------|----------|
| **dbt (core)** | Data modeling, transformations, and orchestration |
| **Snowflake** | Data warehouse / final destination for transformed data |
| **GitHub** | Version control, collaboration, and repository management |
| **GitHub Actions** | Automation, CI/CD workflows |
| **AWS S3** | Public repository containing Bitcoin transaction data (data source) |
| **Python libraries** (`pandas`, `simplejson`) / helper scripts | Ingestion pipeline support |

## ğŸ§© Techniques & Best Practices used

| Technique | Description |
|------------|--------------|
| **Data Quality Tests (singular & generic)** | Ensures data reliability by validating integrity, uniqueness, and referential consistency through both generic tests and reusable (custom generic) dbt tests. |
| **Modularization** | Structured models in layered architecture â€” **staging â†’ marts** â€” improving readability, maintainability, and performance. |
| **Documentation in dbt** | Auto-generate browsable documentation via `dbt docs generate`. |
| **Incremental Models** | Implemented incremental logic in dbt models to process only new or updated records, improving performance and efficiency. |
| **Data Enrichment via dbt Seeds** | Enriched transactional data using static seed files for reference mappings and additional metadata. |
| **Incremental Strategies (Merge & Append)** | Applied `merge` for upserts and `append` for log-based data, depending on data characteristics and freshness needs. |
| **Python Models** | Leveraged Python-based dbt models for complex data transformations or non-SQL operations within the same dbt workflow. |
| **dbt Exposure** | Created **exposures** to explicitly link dbt models to the final **dashboard**, ensuring traceability and visibility of data lineage. |
| **Model Contracts** | Defined **model contracts** to guarantee schema stability; versioning via **dbt versions** was used to manage controlled model evolution. |
| **Continuous Integration (GitHub Actions)** | Implemented CI/CD pipelines with automated testing, linting, and deployment using GitHub Actions to ensure stable, production-ready releases. |


Next Steps: Continuous Loading with Snowpipe - as soon as there is a file uploaded to S3 Bucket -> s3 sends an event to SQS(aws simple queue service) -> Snowpipe will be listening to events on SQS -> as soon as there is a message in SQS snowpipe gets triggered and runs copy into command from stage to btc_raw_table (with a structure already defined).

Could make sense to apply a stream to btc_raw_table which would have (one column data type variant) and use Change Data Capture (CDC) technique to MERGE to final btc_table.

Summary:
Upload to s3 Bucket (sends event to sqs)
Snowpipe is triggered by sqs -> Copy INTO command the data from the stage to raw BTC table
Stream on raw table records changes
Task is triggered to MERGE changes into BTC table (runs when STREAM has data)
Task to trigger dbt prod job (UDF with API call)
GitHub actions ci/cd


The upload of the file to the s3 bucket can also be automated - possible solution AWS Lambda to host and run python script.
